{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, json, math, os, platform, random, sys, time\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich.progress import BarColumn, MofNCompleteColumn, Progress, TextColumn, TimeElapsedColumn, TimeRemainingColumn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import datasets as datasets_lib\n",
    "import transformers as transformers_lib\n",
    "\n",
    "HF_TOKEN: str | None = None\n",
    "GLOBAL_SEED = 42\n",
    "LAMBDA_VALUES = [-5.0, 0.0, 1.0, 5.0]\n",
    "\n",
    "def set_seed(seed_value: int) -> None:\n",
    "\trandom.seed(seed_value)\n",
    "\tos.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "\tnp.random.seed(seed_value)\n",
    "\ttorch.manual_seed(seed_value)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def maybe_configure_hf_auth() -> None:\n",
    "\tif not HF_TOKEN:\n",
    "\t\treturn\n",
    "\tos.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\tos.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "\tos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(GLOBAL_SEED)\n",
    "maybe_configure_hf_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 (BoolQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS_EXPERIMENT_1 = 0\n",
    "\n",
    "BOOLQ_MODELS = [\n",
    "\t\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "\t\"google/gemma-2-2b-it\",\n",
    "\t\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "]\n",
    "\n",
    "def build_vibe_embeddings(model: AutoModelForCausalLM, device: torch.device) -> torch.Tensor:\n",
    "\tembedding_weight = model.get_input_embeddings().weight.detach().to(device=device, dtype=torch.float32)\n",
    "\tnorms = embedding_weight.norm(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "\treturn embedding_weight / norms\n",
    "\n",
    "def compute_label_base_and_alignment(prefix_token_ids: List[int], label_token_ids: List[int], model: AutoModelForCausalLM, vibe_embeddings: torch.Tensor, device: torch.device) -> tuple[float, float]:\n",
    "\ttotal_log_probability = 0.0\n",
    "\ttotal_alignment = 0.0\n",
    "\tprefix = list(prefix_token_ids)\n",
    "\tfor token_id in label_token_ids:\n",
    "\t\tinput_tensor = torch.tensor([prefix], dtype=torch.long, device=device)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(input_ids=input_tensor)\n",
    "\t\t\tlogits = outputs.logits[:, -1, :].to(torch.float32)\n",
    "\t\t\tlog_probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\t\t\tprobabilities = torch.exp(log_probabilities)\n",
    "\t\t\tdirection_vector = probabilities @ vibe_embeddings\n",
    "\t\t\ttoken_vector = vibe_embeddings[token_id]\n",
    "\t\t\talignment_value = torch.dot(token_vector, direction_vector.squeeze(0))\n",
    "\t\t\tlog_probability_token = log_probabilities[0, token_id]\n",
    "\t\ttotal_log_probability += float(log_probability_token.detach().cpu())\n",
    "\t\ttotal_alignment += float(alignment_value.detach().cpu())\n",
    "\t\tprefix.append(int(token_id))\n",
    "\treturn total_log_probability, total_alignment\n",
    "\n",
    "def compute_classification_metrics(true_label_ids: List[int], predictions_for_lambda: Dict[str, List[int]]) -> Dict[str, Dict[str, float]]:\n",
    "\ttrue_tensor = torch.tensor(true_label_ids, dtype=torch.long)\n",
    "\tmetrics: Dict[str, Dict[str, float]] = {}\n",
    "\tfor lambda_key, predicted_ids in predictions_for_lambda.items():\n",
    "\t\tpredicted_tensor = torch.tensor(predicted_ids, dtype=torch.long)\n",
    "\t\tcorrect_count = int((predicted_tensor == true_tensor).sum().item())\n",
    "\t\taccuracy_value = correct_count / len(true_label_ids) if len(true_label_ids) > 0 else 0.0\n",
    "\t\tf1_macro = 0.0\n",
    "\t\tfor class_value in [0, 1]:\n",
    "\t\t\tclass_tensor = torch.full_like(true_tensor, class_value)\n",
    "\t\t\ttrue_positive = int(((true_tensor == class_tensor) & (predicted_tensor == class_tensor)).sum().item())\n",
    "\t\t\tfalse_positive = int(((true_tensor != class_tensor) & (predicted_tensor == class_tensor)).sum().item())\n",
    "\t\t\tfalse_negative = int(((true_tensor == class_tensor) & (predicted_tensor != class_tensor)).sum().item())\n",
    "\t\t\tif true_positive == 0 and false_positive == 0 and false_negative == 0:\n",
    "\t\t\t\tf1_class = 0.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tprecision_value = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0.0\n",
    "\t\t\t\trecall_value = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0.0\n",
    "\t\t\t\tif precision_value + recall_value == 0.0:\n",
    "\t\t\t\t\tf1_class = 0.0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf1_class = 2.0 * precision_value * recall_value / (precision_value + recall_value)\n",
    "\t\t\tf1_macro += f1_class\n",
    "\t\tf1_macro /= 2.0\n",
    "\t\tmetrics[lambda_key] = {\"accuracy\": float(accuracy_value), \"macro_f1\": float(f1_macro)}\n",
    "\treturn metrics\n",
    "\n",
    "def run_boolq_for_model(model_id: str, validation_split, lambda_values: List[float]) -> Dict[str, object]:\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tset_seed(GLOBAL_SEED)\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, token=HF_TOKEN)\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\tvibe_embeddings = build_vibe_embeddings(model, device)\n",
    "\tlabel_texts_by_name = {\"yes\": \" yes\", \"no\": \" no\"}\n",
    "\tlabel_token_ids: Dict[str, List[int]] = {}\n",
    "\tfor label_name, label_text in label_texts_by_name.items():\n",
    "\t\tencoding = tokenizer(label_text, add_special_tokens=False)\n",
    "\t\tlabel_token_ids[label_name] = [int(token_id) for token_id in encoding[\"input_ids\"]]\n",
    "\talpha_keys = [str(value) for value in lambda_values]\n",
    "\tpredictions_by_alpha: Dict[str, List[int]] = {alpha_key: [] for alpha_key in alpha_keys}\n",
    "\ttrue_labels: List[int] = []\n",
    "\texample_records: List[Dict[str, object]] = []\n",
    "\tstart_timestamp = time.time()\n",
    "\tprogress_columns = [\n",
    "\t\tTextColumn(\"{task.description}\"),\n",
    "\t\tBarColumn(),\n",
    "\t\tMofNCompleteColumn(),\n",
    "\t\tTimeElapsedColumn(),\n",
    "\t\tTimeRemainingColumn()\n",
    "\t]\n",
    "\twith Progress(*progress_columns) as progress:\n",
    "\t\ttask_identifier = progress.add_task(f\"Evaluating {model_id}\", total=len(validation_split))\n",
    "\t\tfor row_index in range(len(validation_split)):\n",
    "\t\t\trow = validation_split[int(row_index)]\n",
    "\t\t\tquestion_text = str(row[\"question\"])\n",
    "\t\t\tpassage_text = str(row[\"passage\"])\n",
    "\t\t\tanswer_boolean = bool(row[\"answer\"])\n",
    "\t\t\tgold_label_name = \"yes\" if answer_boolean else \"no\"\n",
    "\t\t\tgold_label_id = 1 if answer_boolean else 0\n",
    "\t\t\ttrue_labels.append(gold_label_id)\n",
    "\t\t\tprompt_text = \"Passage:\\n\" + passage_text + \"\\n\\nQuestion:\\n\" + question_text + \"\\n\\nAnswer the question with a single word: yes or no.\"\n",
    "\t\t\tmessages = [\n",
    "\t\t\t\t{\"role\": \"system\", \"content\": \"You are a question answering assistant. Answer with a single word: \\\"yes\\\" or \\\"no\\\".\"},\n",
    "\t\t\t\t{\"role\": \"user\", \"content\": prompt_text}\n",
    "\t\t\t]\n",
    "\t\t\ttry:\n",
    "\t\t\t\tbase_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\t\t\texcept Exception as template_error:\n",
    "\t\t\t\terror_message = str(template_error)\n",
    "\t\t\t\tif \"System role not supported\" in error_message:\n",
    "\t\t\t\t\tmessages = [\n",
    "\t\t\t\t\t\t{\"role\": \"user\", \"content\": \"You are a question answering assistant. Answer with a single word: \\\"yes\\\" or \\\"no\\\".\\n\\n\" + prompt_text}\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t\tbase_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise\n",
    "\t\t\tencoded_input = tokenizer(base_text, return_tensors=\"pt\")\n",
    "\t\t\tprefix_token_ids = encoded_input[\"input_ids\"][0].tolist()\n",
    "\t\t\tbase_scores: Dict[str, float] = {}\n",
    "\t\t\talignments: Dict[str, float] = {}\n",
    "\t\t\tfor label_name in label_texts_by_name.keys():\n",
    "\t\t\t\tlabel_ids_for_name = label_token_ids[label_name]\n",
    "\t\t\t\tbase_log_probability, alignment_total = compute_label_base_and_alignment(prefix_token_ids, label_ids_for_name, model, vibe_embeddings, device)\n",
    "\t\t\t\tbase_scores[label_name] = float(base_log_probability)\n",
    "\t\t\t\talignments[label_name] = float(alignment_total)\n",
    "\t\t\tpredictions_for_example: Dict[str, Dict[str, object]] = {}\n",
    "\t\t\tfor lambda_value in lambda_values:\n",
    "\t\t\t\tlambda_key = str(lambda_value)\n",
    "\t\t\t\tlabel_scores: Dict[str, float] = {}\n",
    "\t\t\t\tfor label_name in label_texts_by_name.keys():\n",
    "\t\t\t\t\tscore_value = base_scores[label_name] + lambda_value * alignments[label_name]\n",
    "\t\t\t\t\tlabel_scores[label_name] = float(score_value)\n",
    "\t\t\t\tif label_scores[\"yes\"] >= label_scores[\"no\"]:\n",
    "\t\t\t\t\tpredicted_label_name = \"yes\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpredicted_label_name = \"no\"\n",
    "\t\t\t\tpredicted_label_id = 1 if predicted_label_name == \"yes\" else 0\n",
    "\t\t\t\tpredictions_by_alpha[lambda_key].append(predicted_label_id)\n",
    "\t\t\t\tpredictions_for_example[lambda_key] = {\n",
    "\t\t\t\t\t\"predicted_label\": predicted_label_name,\n",
    "\t\t\t\t\t\"scores\": {\"yes\": float(label_scores[\"yes\"]), \"no\": float(label_scores[\"no\"])}\n",
    "\t\t\t\t}\n",
    "\t\t\texample_records.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"index\": int(row_index),\n",
    "\t\t\t\t\t\"question\": question_text,\n",
    "\t\t\t\t\t\"passage\": passage_text,\n",
    "\t\t\t\t\t\"gold_label\": gold_label_name,\n",
    "\t\t\t\t\t\"input_messages\": messages,\n",
    "\t\t\t\t\t\"input_text\": base_text,\n",
    "\t\t\t\t\t\"label_token_ids\": {key: [int(value) for value in values] for key, values in label_token_ids.items()},\n",
    "\t\t\t\t\t\"predictions\": predictions_for_example\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\t\t\tprogress.advance(task_identifier, 1)\n",
    "\tend_timestamp = time.time()\n",
    "\tmetrics = compute_classification_metrics(true_labels, predictions_by_alpha)\n",
    "\tsetup_data = {\n",
    "\t\t\"model_name\": model_id,\n",
    "\t\t\"dataset_name\": \"google/boolq\",\n",
    "\t\t\"split\": \"validation\",\n",
    "\t\t\"lambda_values\": [float(value) for value in lambda_values],\n",
    "\t\t\"max_rows\": int(MAX_ROWS_EXPERIMENT_1),\n",
    "\t\t\"num_examples_evaluated\": int(len(true_labels)),\n",
    "\t\t\"random_seed\": GLOBAL_SEED,\n",
    "\t\t\"device\": str(device),\n",
    "\t\t\"vibe_method\": \"Label scoring with sequential vibe-adjusted token log-probabilities for labels \\\"yes\\\" and \\\"no\\\".\",\n",
    "\t\t\"label_texts_by_name\": label_texts_by_name,\n",
    "\t\t\"library_versions\": {\n",
    "\t\t\t\"python\": platform.python_version(),\n",
    "\t\t\t\"torch\": str(torch.__version__),\n",
    "\t\t\t\"transformers\": str(transformers_lib.__version__),\n",
    "\t\t\t\"datasets\": str(datasets_lib.__version__)\n",
    "\t\t},\n",
    "\t\t\"runtime_seconds\": float(end_timestamp - start_timestamp),\n",
    "\t\t\"remarks\": f\"{model_id} evaluated on BoolQ using vibe decoding over label strings.\"\n",
    "\t}\n",
    "\tresults_data = {\n",
    "\t\t\"model_name\": model_id,\n",
    "\t\t\"dataset_name\": \"google/boolq\",\n",
    "\t\t\"split\": \"validation\",\n",
    "\t\t\"lambda_values\": [float(value) for value in lambda_values],\n",
    "\t\t\"metrics\": metrics,\n",
    "\t\t\"examples\": example_records\n",
    "\t}\n",
    "\treturn {\"setup\": setup_data, \"results\": results_data}\n",
    "\n",
    "dataset_boolq = load_dataset(\"google/boolq\")\n",
    "validation_dataset = dataset_boolq[\"validation\"]\n",
    "if MAX_ROWS_EXPERIMENT_1 and MAX_ROWS_EXPERIMENT_1 > 0:\n",
    "\trow_count = min(MAX_ROWS_EXPERIMENT_1, len(validation_dataset))\n",
    "\tvalidation_dataset = validation_dataset.select(range(row_count))\n",
    "\n",
    "experiment_1_output: Dict[str, object] = {\n",
    "\t\"experiment_id\": 1,\n",
    "\t\"task\": \"BoolQ yes or no classification with vibe decoding over label strings.\",\n",
    "\t\"lambda_values\": [float(value) for value in LAMBDA_VALUES],\n",
    "\t\"random_seed\": GLOBAL_SEED,\n",
    "\t\"models\": {}\n",
    "}\n",
    "\n",
    "for model_identifier in BOOLQ_MODELS:\n",
    "\texperiment_1_output[\"models\"][model_identifier] = run_boolq_for_model(model_identifier, validation_dataset, LAMBDA_VALUES)\n",
    "\n",
    "with open(\"experiment-1.json\", \"w\", encoding=\"utf-8\") as file_handle:\n",
    "\tjson.dump(experiment_1_output, file_handle, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 (NumerSense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS_EXPERIMENT_2 = 0\n",
    "\n",
    "CANDIDATE_WORDS = [\n",
    "\t\"zero\",\n",
    "\t\"one\",\n",
    "\t\"two\",\n",
    "\t\"three\",\n",
    "\t\"four\",\n",
    "\t\"five\",\n",
    "\t\"six\",\n",
    "\t\"seven\",\n",
    "\t\"eight\",\n",
    "\t\"nine\",\n",
    "\t\"ten\"\n",
    "]\n",
    "\n",
    "NUMERSENSE_DATASET_ID = \"INK-USC/numer_sense\"\n",
    "NUMERSENSE_FALLBACK_TSV_URL = \"https://raw.githubusercontent.com/INK-USC/NumerSense/main/data/validation.masked.tsv\"\n",
    "\n",
    "SYSTEM_PROMPT_NUMERSENSE = \"You are a helpful assistant.\"\n",
    "USER_TEMPLATE_NUMERSENSE = (\n",
    "\t\"You are given a sentence where a number between 0 and 10 has been replaced by the token <mask>.\\n\"\n",
    "\t\"Sentence: {sentence}\\n\\n\"\n",
    "\t\"Which single English word from [zero, one, two, three, four, five, six, seven, eight, nine, ten] best fills in for <mask>? Answer with just the word.\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ExampleResult:\n",
    "\tindex: int\n",
    "\tx: str\n",
    "\ty: str\n",
    "\ttarget: str\n",
    "\tlmb: float\n",
    "\thit1: int\n",
    "\trank: int | None\n",
    "\n",
    "NUMERSENSE_MODELS = [\n",
    "\t{\"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\", \"use_system_prompt\": True},\n",
    "\t{\"model_id\": \"google/gemma-2-2b-it\", \"use_system_prompt\": False},\n",
    "\t{\"model_id\": \"meta-llama/Llama-3.1-8B-Instruct\", \"use_system_prompt\": True}\n",
    "]\n",
    "\n",
    "def prepare_tokenizer_and_model_numer_sense(model_id: str) -> tuple[AutoTokenizer, AutoModelForCausalLM, torch.device]:\n",
    "\ttoken_kwargs: Dict[str, object] = {}\n",
    "\tif HF_TOKEN:\n",
    "\t\ttoken_kwargs[\"token\"] = HF_TOKEN\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_id, **token_kwargs)\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", **token_kwargs)\n",
    "\tdevice = model.device\n",
    "\tmodel.eval()\n",
    "\treturn tokenizer, model, device\n",
    "\n",
    "def apply_chat_template_numer_sense(tokenizer: AutoTokenizer, sentence: str, use_system_prompt: bool) -> str:\n",
    "\tcontent = USER_TEMPLATE_NUMERSENSE.format(sentence=sentence)\n",
    "\tif use_system_prompt:\n",
    "\t\tmessages = [\n",
    "\t\t\t{\"role\": \"system\", \"content\": SYSTEM_PROMPT_NUMERSENSE},\n",
    "\t\t\t{\"role\": \"user\", \"content\": content}\n",
    "\t\t]\n",
    "\telse:\n",
    "\t\tmessages = [\n",
    "\t\t\t{\"role\": \"user\", \"content\": content}\n",
    "\t\t]\n",
    "\treturn tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def get_lm_head_and_norms(model: AutoModelForCausalLM) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "\tlm_head = model.get_output_embeddings()\n",
    "\tweight_matrix = lm_head.weight.detach()\n",
    "\tif weight_matrix.dim() != 2:\n",
    "\t\traise ValueError(\"Language model head weight must be rank 2.\")\n",
    "\twith torch.no_grad():\n",
    "\t\tnorms = torch.linalg.norm(weight_matrix.float(), dim=1)\n",
    "\t\tinverse_norms = torch.where(norms > 0, norms.reciprocal(), torch.zeros_like(norms))\n",
    "\t\tinverse_norms = inverse_norms.to(weight_matrix.dtype)\n",
    "\treturn weight_matrix, inverse_norms\n",
    "\n",
    "def build_candidate_token_ids(tokenizer: AutoTokenizer) -> Dict[str, int]:\n",
    "\tcandidate_ids: Dict[str, int] = {}\n",
    "\tfor word in CANDIDATE_WORDS:\n",
    "\t\ttoken_ids_with_space = tokenizer(\" \" + word, add_special_tokens=False)[\"input_ids\"]\n",
    "\t\tif len(token_ids_with_space) == 1:\n",
    "\t\t\tcandidate_ids[word] = token_ids_with_space[0]\n",
    "\t\t\tcontinue\n",
    "\t\ttoken_ids_plain = tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
    "\t\tif len(token_ids_plain) == 1:\n",
    "\t\t\tcandidate_ids[word] = token_ids_plain[0]\n",
    "\t\t\tcontinue\n",
    "\t\traise ValueError(f\"Cannot map word {word!r} to a single token.\")\n",
    "\treturn candidate_ids\n",
    "\n",
    "def first_step_scores(base_logits: torch.Tensor, weight_matrix: torch.Tensor, inverse_norms: torch.Tensor, lambda_value: float) -> torch.Tensor:\n",
    "\tif base_logits.dim() != 1:\n",
    "\t\traise ValueError(\"Base logits for NumerSense must be one dimensional.\")\n",
    "\tif base_logits.shape[0] != weight_matrix.shape[0]:\n",
    "\t\traise ValueError(\"Base logits and language model head weight dimension mismatch.\")\n",
    "\tif base_logits.dtype != weight_matrix.dtype:\n",
    "\t\tbase_logits = base_logits.to(weight_matrix.dtype)\n",
    "\tvocabulary_size, hidden_dim = weight_matrix.shape\n",
    "\tif inverse_norms.shape[0] != vocabulary_size:\n",
    "\t\traise ValueError(\"Inverse norms shape does not match vocabulary size.\")\n",
    "\tprobabilities = torch.softmax(base_logits.float(), dim=-1)\n",
    "\tweighted_probabilities = probabilities * inverse_norms.float()\n",
    "\tdirection = weighted_probabilities @ weight_matrix.float()\n",
    "\te_dot_direction = (weight_matrix.float() @ direction) * inverse_norms.float()\n",
    "\tif lambda_value != 0.0:\n",
    "\t\te_dot_direction = e_dot_direction.to(base_logits.dtype)\n",
    "\t\tadjusted = base_logits + lambda_value * e_dot_direction\n",
    "\t\treturn adjusted\n",
    "\treturn base_logits\n",
    "\n",
    "def load_numer_sense_data(max_rows: int) -> List[Dict[str, object]]:\n",
    "\ttry:\n",
    "\t\tdataset_split = load_dataset(NUMERSENSE_DATASET_ID, split=\"train\", trust_remote_code=True)\n",
    "\texcept Exception:\n",
    "\t\tdataset_split = load_dataset(\"csv\", data_files={\"train\": NUMERSENSE_FALLBACK_TSV_URL}, sep=\"\\t\")[\"train\"]\n",
    "\tif max_rows and max_rows > 0:\n",
    "\t\trow_count = min(max_rows, len(dataset_split))\n",
    "\t\tdataset_split = dataset_split.select(range(row_count))\n",
    "\trows: List[Dict[str, object]] = []\n",
    "\tcandidate_set = set(CANDIDATE_WORDS)\n",
    "\tcolumn_names = list(dataset_split.column_names)\n",
    "\tdef try_named_columns(item: Dict[str, object]) -> tuple[str | None, str | None]:\n",
    "\t\tsentence_value = None\n",
    "\t\tfor key in [\"sentence\", \"probe\", \"text\", \"input\"]:\n",
    "\t\t\tif key in item and isinstance(item[key], str):\n",
    "\t\t\t\tsentence_value = item[key]\n",
    "\t\t\t\tbreak\n",
    "\t\ttarget_value = None\n",
    "\t\tfor key in [\"target\", \"answer\", \"label\", \"ground_truth\", \"gold\"]:\n",
    "\t\t\tif key in item and isinstance(item[key], str):\n",
    "\t\t\t\ttarget_value = item[key]\n",
    "\t\t\t\tbreak\n",
    "\t\treturn sentence_value, target_value\n",
    "\tdef autodetect_columns() -> tuple[str, str]:\n",
    "\t\tsentence_column_name = None\n",
    "\t\ttarget_column_name = None\n",
    "\t\tmask_counts: Dict[str, int] = {}\n",
    "\t\tcandidate_counts: Dict[str, int] = {}\n",
    "\t\tfor column_name in column_names:\n",
    "\t\t\tcolumn_values = dataset_split[column_name]\n",
    "\t\t\tif all(isinstance(value, str) for value in column_values):\n",
    "\t\t\t\tmask_counts[column_name] = sum(1 for value in column_values if \"<mask>\" in value)\n",
    "\t\t\t\tcandidate_counts[column_name] = sum(1 for value in column_values if value.strip().lower() in candidate_set)\n",
    "\t\tif mask_counts:\n",
    "\t\t\tmax_mask_column_name = max(mask_counts, key=lambda name: mask_counts[name])\n",
    "\t\t\tif mask_counts[max_mask_column_name] > 0:\n",
    "\t\t\t\tsentence_column_name = max_mask_column_name\n",
    "\t\tif candidate_counts:\n",
    "\t\t\tmax_candidate_column_name = max(candidate_counts, key=lambda name: candidate_counts[name])\n",
    "\t\t\tif candidate_counts[max_candidate_column_name] > 0:\n",
    "\t\t\t\ttarget_column_name = max_candidate_column_name\n",
    "\t\tif not sentence_column_name or not target_column_name:\n",
    "\t\t\traise ValueError(\"Could not locate sentence or target column for NumerSense.\")\n",
    "\t\treturn sentence_column_name, target_column_name\n",
    "\tsentence_column_name: str | None = None\n",
    "\ttarget_column_name: str | None = None\n",
    "\tfirst_item = dataset_split[0] if len(dataset_split) > 0 else {}\n",
    "\tprobe_sentence, probe_target = try_named_columns(first_item) if first_item else (None, None)\n",
    "\tif probe_sentence and probe_target:\n",
    "\t\tsentence_column_name = next(key for key in column_names if key in first_item and first_item[key] == probe_sentence)\n",
    "\t\ttarget_column_name = next(key for key in column_names if key in first_item and first_item[key] == probe_target)\n",
    "\telse:\n",
    "\t\tsentence_column_name, target_column_name = autodetect_columns()\n",
    "\tfor index in range(len(dataset_split)):\n",
    "\t\titem = dataset_split[index]\n",
    "\t\tsentence_text = str(item[sentence_column_name])\n",
    "\t\ttarget_text = str(item[target_column_name]).strip().lower()\n",
    "\t\trows.append({\"index\": index, \"sentence\": sentence_text, \"target\": target_text})\n",
    "\treturn rows\n",
    "\n",
    "def evaluate_lambda_for_numer_sense(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, device: torch.device, data_rows: List[Dict[str, object]], lambda_value: float, weight_matrix: torch.Tensor, inverse_norms: torch.Tensor, candidate_ids: Dict[str, int], use_system_prompt: bool) -> tuple[List[ExampleResult], float, float]:\n",
    "\tresults: List[ExampleResult] = []\n",
    "\thits = 0\n",
    "\treciprocal_rank_sum = 0.0\n",
    "\treciprocal_rank_count = 0\n",
    "\ttotal = len(data_rows)\n",
    "\tfor row in tqdm(data_rows, desc=f\"λ={lambda_value:+g}\"):\n",
    "\t\tindex = int(row[\"index\"])\n",
    "\t\tsentence = str(row[\"sentence\"])\n",
    "\t\ttarget = str(row[\"target\"])\n",
    "\t\tprompt_text = apply_chat_template_numer_sense(tokenizer, sentence, use_system_prompt)\n",
    "\t\ttokenised = tokenizer(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\t\tinput_ids = tokenised[\"input_ids\"].to(device)\n",
    "\t\tattention_mask = tokenised.get(\"attention_mask\")\n",
    "\t\tif attention_mask is not None:\n",
    "\t\t\tattention_mask = attention_mask.to(device)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\tlogits = outputs.logits\n",
    "\t\tif logits.dim() != 3:\n",
    "\t\t\traise ValueError(\"NumerSense logits must be three dimensional.\")\n",
    "\t\tbase_logits = logits[0, -1, :]\n",
    "\t\tif base_logits.shape[0] != weight_matrix.shape[0]:\n",
    "\t\t\traise ValueError(\"NumerSense base logits vocabulary mismatch.\")\n",
    "\t\tadjusted_logits = first_step_scores(base_logits, weight_matrix, inverse_norms, lambda_value)\n",
    "\t\tscore_items: List[tuple[str, float]] = []\n",
    "\t\tfor word, token_id in candidate_ids.items():\n",
    "\t\t\tscore_items.append((word, float(adjusted_logits[token_id].item())))\n",
    "\t\tscore_items.sort(key=lambda pair: pair[1], reverse=True)\n",
    "\t\tpredicted_word = score_items[0][0]\n",
    "\t\thit_value = 1 if predicted_word == target else 0\n",
    "\t\thits += hit_value\n",
    "\t\trank_value: int | None = None\n",
    "\t\tif target in candidate_ids:\n",
    "\t\t\tordered_words = [name for name, _ in score_items]\n",
    "\t\t\tif target in ordered_words:\n",
    "\t\t\t\trank_value = 1 + ordered_words.index(target)\n",
    "\t\t\t\treciprocal_rank_sum += 1.0 / rank_value\n",
    "\t\t\t\treciprocal_rank_count += 1\n",
    "\t\tresults.append(\n",
    "\t\t\tExampleResult(\n",
    "\t\t\t\tindex=index,\n",
    "\t\t\t\tx=prompt_text,\n",
    "\t\t\t\ty=predicted_word,\n",
    "\t\t\t\ttarget=target,\n",
    "\t\t\t\tlmb=lambda_value,\n",
    "\t\t\t\thit1=hit_value,\n",
    "\t\t\t\trank=rank_value\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\thit_at_1 = hits / max(1, total)\n",
    "\tif reciprocal_rank_count == 0:\n",
    "\t\tmean_reciprocal_rank = 0.0\n",
    "\t\treturn results, hit_at_1, mean_reciprocal_rank\n",
    "\tmean_reciprocal_rank = reciprocal_rank_sum / reciprocal_rank_count\n",
    "\treturn results, hit_at_1, mean_reciprocal_rank\n",
    "\n",
    "def run_numer_sense_for_model(model_id: str, use_system_prompt: bool, lambda_values: List[float], max_rows: int) -> Dict[str, object]:\n",
    "\tset_seed(GLOBAL_SEED)\n",
    "\ttokenizer, model, device = prepare_tokenizer_and_model_numer_sense(model_id)\n",
    "\tweight_matrix, inverse_norms = get_lm_head_and_norms(model)\n",
    "\tweight_matrix = weight_matrix.to(device)\n",
    "\tinverse_norms = inverse_norms.to(device)\n",
    "\tcandidate_ids = build_candidate_token_ids(tokenizer)\n",
    "\tdata_rows = load_numer_sense_data(max_rows)\n",
    "\tall_records: List[Dict[str, object]] = []\n",
    "\tmetrics: Dict[str, Dict[str, str]] = {}\n",
    "\tfor lambda_value in lambda_values:\n",
    "\t\trun_results, hit_at_1, mean_reciprocal_rank = evaluate_lambda_for_numer_sense(model, tokenizer, device, data_rows, lambda_value, weight_matrix, inverse_norms, candidate_ids, use_system_prompt)\n",
    "\t\tprint(f\"Model={model_id} λ={lambda_value:+g} hit@1={hit_at_1:.8f} MRR={mean_reciprocal_rank:.8f}\", flush=True)\n",
    "\t\tmetrics[str(lambda_value)] = {\"hit@1\": f\"{hit_at_1:.8f}\", \"MRR\": f\"{mean_reciprocal_rank:.8f}\"}\n",
    "\t\tfor result in run_results:\n",
    "\t\t\tall_records.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"index\": result.index,\n",
    "\t\t\t\t\t\"x\": result.x,\n",
    "\t\t\t\t\t\"y\": result.y,\n",
    "\t\t\t\t\t\"target\": result.target,\n",
    "\t\t\t\t\t\"lambda\": result.lmb,\n",
    "\t\t\t\t\t\"hit@1\": result.hit1,\n",
    "\t\t\t\t\t\"rank\": result.rank\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\treturn {\"metrics\": metrics, \"records\": all_records}\n",
    "\n",
    "experiment_2_output: Dict[str, object] = {\n",
    "\t\"experiment_id\": 2,\n",
    "\t\"task\": \"NumerSense masked number prediction with vibe decoding.\",\n",
    "\t\"lambda_values\": [float(value) for value in LAMBDA_VALUES],\n",
    "\t\"random_seed\": GLOBAL_SEED,\n",
    "\t\"dataset\": {\n",
    "\t\t\"id\": NUMERSENSE_DATASET_ID,\n",
    "\t\t\"fallback_tsv_url\": NUMERSENSE_FALLBACK_TSV_URL,\n",
    "\t\t\"max_rows\": int(MAX_ROWS_EXPERIMENT_2)\n",
    "\t},\n",
    "\t\"models\": {}\n",
    "}\n",
    "\n",
    "for model_config in NUMERSENSE_MODELS:\n",
    "\tmodel_identifier = model_config[\"model_id\"]\n",
    "\tuse_system = bool(model_config[\"use_system_prompt\"])\n",
    "\texperiment_2_output[\"models\"][model_identifier] = run_numer_sense_for_model(model_identifier, use_system, LAMBDA_VALUES, MAX_ROWS_EXPERIMENT_2)\n",
    "\n",
    "with open(\"experiment-2.json\", \"w\", encoding=\"utf-8\") as file_handle:\n",
    "\tjson.dump(experiment_2_output, file_handle, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 (PG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS_EXPERIMENT_3 = 10\n",
    "PG19_BLOCK_LENGTH = 1024\n",
    "PG19_BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"1\"))\n",
    "PG19_DATASET_ID = \"emozilla/pg19\"\n",
    "PG19_SPLIT = \"test\"\n",
    "\n",
    "PG19_MODELS = [\n",
    "\t\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "\t\"google/gemma-2-2b-it\",\n",
    "\t\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "]\n",
    "\n",
    "def select_pg19_dtype() -> torch.dtype:\n",
    "\tdtype_flag = os.environ.get(\"DTYPE\", \"\").lower()\n",
    "\tif dtype_flag == \"bf16\":\n",
    "\t\treturn torch.bfloat16\n",
    "\tif dtype_flag == \"fp16\":\n",
    "\t\treturn torch.float16\n",
    "\tif torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "\t\treturn torch.bfloat16\n",
    "\treturn torch.float16\n",
    "\n",
    "def get_pg19_text_column(dataset_split) -> str:\n",
    "\tif isinstance(dataset_split.column_names, list):\n",
    "\t\tcolumn_names = dataset_split.column_names\n",
    "\telse:\n",
    "\t\tcolumn_names = list(dataset_split.column_names)\n",
    "\tif \"text\" in column_names:\n",
    "\t\treturn \"text\"\n",
    "\tif \"book_text\" in column_names:\n",
    "\t\treturn \"book_text\"\n",
    "\traise KeyError(f\"No suitable text column found. Columns: {column_names}\")\n",
    "\n",
    "def get_pg19_model_max_length(tokenizer: AutoTokenizer, model: AutoModelForCausalLM, fallback_max_length: int = 131072) -> int:\n",
    "\tmax_length_value = getattr(model.config, \"max_position_embeddings\", None)\n",
    "\tif max_length_value is None or max_length_value <= 0:\n",
    "\t\tmax_length_value = getattr(tokenizer, \"model_max_length\", None)\n",
    "\tif max_length_value is None or max_length_value <= 0 or max_length_value > 10_000_000:\n",
    "\t\tmax_length_value = fallback_max_length\n",
    "\treturn int(max_length_value)\n",
    "\n",
    "def prepare_pg19_dataset(tokenizer: AutoTokenizer, texts: List[str], block_length: int, model_max_length: int) -> List[torch.Tensor]:\n",
    "\tblock_size = max(2, min(block_length, model_max_length - 1))\n",
    "\tchunks: List[torch.Tensor] = []\n",
    "\tfor text_value in texts:\n",
    "\t\tif not text_value:\n",
    "\t\t\tcontinue\n",
    "\t\tencoding = tokenizer(text_value, add_special_tokens=False)\n",
    "\t\tinput_ids = encoding[\"input_ids\"]\n",
    "\t\tsequence_length = len(input_ids)\n",
    "\t\tif sequence_length < 2:\n",
    "\t\t\tcontinue\n",
    "\t\tfor start_index in range(0, sequence_length, block_size):\n",
    "\t\t\tchunk_ids = input_ids[start_index:start_index + block_size]\n",
    "\t\t\tif len(chunk_ids) >= 2:\n",
    "\t\t\t\tchunks.append(torch.tensor(chunk_ids, dtype=torch.long))\n",
    "\treturn chunks\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_pg19_with_lambda(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, chunks: List[torch.Tensor], lambda_value: float, device_identifier: str, dtype: torch.dtype, model_max_length: int) -> float:\n",
    "\tembedding_weight = model.get_input_embeddings().weight.to(device=device_identifier, dtype=dtype)\n",
    "\tvocabulary_size, hidden_dimension = embedding_weight.shape\n",
    "\tembedding_float = embedding_weight.detach().to(torch.float32)\n",
    "\tnorms = torch.linalg.norm(embedding_float, dim=1, keepdim=True).clamp_min(1e-12)\n",
    "\tvibe_matrix = embedding_float / norms\n",
    "\tvibe_matrix_transposed = vibe_matrix.t().contiguous()\n",
    "\ttotal_negative_log_likelihood = 0.0\n",
    "\ttotal_tokens = 0\n",
    "\tmodel.eval()\n",
    "\tprogress_bar = tqdm(total=len(chunks), desc=f\"λ={lambda_value:.1f}\")\n",
    "\tfor start_index in range(0, len(chunks), PG19_BATCH_SIZE):\n",
    "\t\tbatch_chunks = chunks[start_index:start_index + PG19_BATCH_SIZE]\n",
    "\t\tbatch_chunks = [sequence[:model_max_length] if sequence.size(0) > model_max_length else sequence for sequence in batch_chunks]\n",
    "\t\tmax_length_in_batch = max(sequence.size(0) for sequence in batch_chunks)\n",
    "\t\tpad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\t\tinput_ids = torch.full((len(batch_chunks), max_length_in_batch), pad_token_id, dtype=torch.long)\n",
    "\t\tfor batch_index, sequence in enumerate(batch_chunks):\n",
    "\t\t\tinput_ids[batch_index, :sequence.size(0)] = sequence\n",
    "\t\tattention_mask = (input_ids != pad_token_id).long()\n",
    "\t\tinput_ids = input_ids.to(device_identifier)\n",
    "\t\tattention_mask = attention_mask.to(device_identifier)\n",
    "\t\toutput = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\tlogits = output.logits\n",
    "\t\tbatch_size_value, time_value, vocab_dimension = logits.shape\n",
    "\t\tif vocab_dimension != vocabulary_size:\n",
    "\t\t\traise RuntimeError(\"Vocabulary size mismatch between embedding matrix and logits.\")\n",
    "\t\tlogits_pred = logits[:, :-1, :].contiguous()\n",
    "\t\tlabels = input_ids[:, 1:].contiguous()\n",
    "\t\tattention_sub_mask = attention_mask[:, 1:].contiguous()\n",
    "\t\tbatch_time = logits_pred.shape[0] * logits_pred.shape[1]\n",
    "\t\tlogits_flat = logits_pred.reshape(batch_time, vocabulary_size).to(torch.float32)\n",
    "\t\ttoken_probabilities = torch.softmax(logits_flat, dim=-1)\n",
    "\t\tvibe_direction = token_probabilities @ vibe_matrix\n",
    "\t\tvibe_scores = vibe_direction @ vibe_matrix_transposed\n",
    "\t\tadjusted_logits = logits_flat + lambda_value * vibe_scores\n",
    "\t\tlog_probabilities = torch.log_softmax(adjusted_logits, dim=-1)\n",
    "\t\ttarget_flat = labels.reshape(batch_time)\n",
    "\t\tmask_flat = attention_sub_mask.reshape(batch_time).to(torch.float32)\n",
    "\t\tgold_log_probabilities = log_probabilities.gather(1, target_flat.unsqueeze(1)).squeeze(1)\n",
    "\t\tnegative_log_likelihood = -(gold_log_probabilities * mask_flat).sum().item()\n",
    "\t\ttoken_count = int(mask_flat.sum().item())\n",
    "\t\ttotal_negative_log_likelihood += negative_log_likelihood\n",
    "\t\ttotal_tokens += token_count\n",
    "\t\tprogress_bar.update(len(batch_chunks))\n",
    "\tprogress_bar.close()\n",
    "\tif total_tokens == 0:\n",
    "\t\treturn float(\"inf\")\n",
    "\treturn float(math.exp(total_negative_log_likelihood / total_tokens))\n",
    "\n",
    "def run_pg19_for_model(model_id: str) -> Dict[str, float]:\n",
    "\tset_seed(GLOBAL_SEED)\n",
    "\tdevice_identifier = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\tdtype = select_pg19_dtype()\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "\tif tokenizer.pad_token is None:\n",
    "\t\ttokenizer.pad_token = tokenizer.eos_token\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_id, token=HF_TOKEN, device_map=\"auto\", torch_dtype=dtype)\n",
    "\tmodel_max_length = get_pg19_model_max_length(tokenizer, model)\n",
    "\ttokenizer.model_max_length = 10_000_000_000\n",
    "\tsafe_block_length = min(PG19_BLOCK_LENGTH, model_max_length - 1)\n",
    "\tdataset_split = load_dataset(PG19_DATASET_ID, split=PG19_SPLIT)\n",
    "\tif MAX_ROWS_EXPERIMENT_3:\n",
    "\t\trow_count = min(MAX_ROWS_EXPERIMENT_3, len(dataset_split))\n",
    "\t\tdataset_split = dataset_split.select(range(row_count))\n",
    "\ttext_column_name = get_pg19_text_column(dataset_split)\n",
    "\ttexts = dataset_split[text_column_name]\n",
    "\tchunks = prepare_pg19_dataset(tokenizer, texts, safe_block_length, model_max_length)\n",
    "\tif not chunks:\n",
    "\t\tprint(f\"No usable chunks from dataset for model {model_id}.\")\n",
    "\t\treturn {}\n",
    "\tresults: Dict[str, float] = {}\n",
    "\tfor lambda_value in LAMBDA_VALUES:\n",
    "\t\tperplexity_value = evaluate_pg19_with_lambda(model, tokenizer, chunks, lambda_value, device_identifier, dtype, model_max_length)\n",
    "\t\tperplexity_string = f\"{perplexity_value:.8f}\"\n",
    "\t\tresults[str(lambda_value)] = float(perplexity_string)\n",
    "\t\tprint(f\"[{model_id}] λ={lambda_value:+}: perplexity={perplexity_string}\")\n",
    "\treturn results\n",
    "\n",
    "experiment_3_output: Dict[str, object] = {\n",
    "\t\"experiment_id\": 3,\n",
    "\t\"task\": \"PG19 perplexity with vibe-adjusted decoding.\",\n",
    "\t\"lambda_values\": [float(value) for value in LAMBDA_VALUES],\n",
    "\t\"random_seed\": GLOBAL_SEED,\n",
    "\t\"dataset\": {\n",
    "\t\t\"id\": PG19_DATASET_ID,\n",
    "\t\t\"split\": PG19_SPLIT,\n",
    "\t\t\"block_length\": PG19_BLOCK_LENGTH,\n",
    "\t\t\"batch_size\": PG19_BATCH_SIZE,\n",
    "\t\t\"max_rows\": int(MAX_ROWS_EXPERIMENT_3)\n",
    "\t},\n",
    "\t\"models\": {}\n",
    "}\n",
    "\n",
    "for model_identifier in PG19_MODELS:\n",
    "\texperiment_3_output[\"models\"][model_identifier] = run_pg19_for_model(model_identifier)\n",
    "\n",
    "with open(\"experiment-3.json\", \"w\", encoding=\"utf-8\") as file_handle:\n",
    "\tjson.dump(experiment_3_output, file_handle, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 (WikiText-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS_EXPERIMENT_4 = 1000\n",
    "WIKITEXT_MODELS = [\n",
    "\t\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "\t\"google/gemma-2-2b-it\",\n",
    "\t\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "]\n",
    "WIKITEXT_DATASET_ID = \"Salesforce/wikitext\"\n",
    "WIKITEXT_CONFIG = \"wikitext-2-v1\"\n",
    "WIKITEXT_SPLIT = \"validation\"\n",
    "WIKITEXT_TIME_SLICE = 32\n",
    "WIKITEXT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WIKITEXT_PREFERRED_DTYPE = torch.bfloat16 if (WIKITEXT_DEVICE == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.backends.cuda.matmul.allow_tf32 = True\n",
    "\ttorch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "def require(condition: bool, message: str) -> None:\n",
    "\tif condition:\n",
    "\t\treturn\n",
    "\traise ValueError(message)\n",
    "\n",
    "def get_tokenizer_and_model_wikitext(model_id: str, torch_dtype: torch.dtype) -> tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, token=HF_TOKEN)\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(\n",
    "\t\tmodel_id,\n",
    "\t\ttorch_dtype=torch_dtype,\n",
    "\t\tdevice_map=\"auto\" if WIKITEXT_DEVICE == \"cuda\" else None,\n",
    "\t\ttoken=HF_TOKEN\n",
    "\t)\n",
    "\tif WIKITEXT_DEVICE != \"cuda\":\n",
    "\t\tmodel.to(WIKITEXT_DEVICE)\n",
    "\tmodel.eval()\n",
    "\treturn tokenizer, model\n",
    "\n",
    "def check_shapes_and_dtypes(model: AutoModelForCausalLM) -> tuple[int, int, torch.dtype]:\n",
    "\tembedding_matrix = model.get_input_embeddings().weight\n",
    "\tif embedding_matrix.ndim != 2:\n",
    "\t\traise ValueError(f\"Embedding matrix must be rank 2, got {embedding_matrix.shape}\")\n",
    "\tvocabulary_size, hidden_dimension = embedding_matrix.shape\n",
    "\tif embedding_matrix.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n",
    "\t\traise ValueError(f\"Unexpected embedding dtype {embedding_matrix.dtype}\")\n",
    "\treturn vocabulary_size, hidden_dimension, embedding_matrix.dtype\n",
    "\n",
    "def verify_dataset_structure(dataset_wikitext, split_name: str) -> None:\n",
    "\tif split_name not in dataset_wikitext:\n",
    "\t\traise ValueError(f\"Missing split {split_name} in Wikitext dataset.\")\n",
    "\tfeatures = dataset_wikitext[split_name].features\n",
    "\tif \"text\" not in features or str(features[\"text\"].dtype) != \"string\":\n",
    "\t\traise ValueError(f\"Expected 'text' feature of type string, got {features}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_vibe_matrix(model: AutoModelForCausalLM, device_identifier: str) -> torch.Tensor:\n",
    "\tweight_matrix = model.get_input_embeddings().weight.detach()\n",
    "\tembeddings = weight_matrix.to(torch.float32)\n",
    "\tnorms = embeddings.norm(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "\tembeddings = embeddings / norms\n",
    "\treturn embeddings.to(device_identifier, non_blocking=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vibe_adjusted_logprobs(logits: torch.Tensor, vibe_matrix: torch.Tensor, lambda_value: float) -> torch.Tensor:\n",
    "\tif logits.ndim == 1:\n",
    "\t\tlogits = logits.unsqueeze(0)\n",
    "\tbatch_size, vocabulary_size = logits.shape\n",
    "\tif vocabulary_size != vibe_matrix.shape[0]:\n",
    "\t\traise ValueError(\"Vocab mismatch between logits and vibe matrix.\")\n",
    "\tlogits = logits.to(torch.float32)\n",
    "\tlog_probs = torch.log_softmax(logits, dim=-1)\n",
    "\tprobabilities = torch.exp(log_probs)\n",
    "\tdirections = torch.matmul(vibe_matrix.t(), probabilities.t())\n",
    "\tbias = torch.matmul(vibe_matrix, directions).t()\n",
    "\tscores = log_probs + lambda_value * bias\n",
    "\treturn scores - torch.logsumexp(scores, dim=-1, keepdim=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def stride_ppl_with_vibe(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, vibe_matrix: torch.Tensor, input_ids: torch.Tensor, lambda_value: float, max_length: int, stride: int = 512) -> float:\n",
    "\tnegative_log_likelihood_sum = 0.0\n",
    "\ttoken_count_total = 0\n",
    "\tsteps = list(range(0, input_ids.size(1), stride))\n",
    "\tfor start_index in tqdm(steps, desc=f\"PPL λ={lambda_value:+g}\", leave=False):\n",
    "\t\tend_index = min(start_index + max_length, input_ids.size(1))\n",
    "\t\tif end_index - start_index <= 1:\n",
    "\t\t\tbreak\n",
    "\t\tchunk_ids = input_ids[:, start_index:end_index].to(WIKITEXT_DEVICE, non_blocking=True)\n",
    "\t\toutputs = model(input_ids=chunk_ids)\n",
    "\t\tlogits = outputs.logits[:, :-1, :]\n",
    "\t\ttarget_ids = chunk_ids[:, 1:]\n",
    "\t\tbatch_size, time_steps, vocabulary_size = logits.shape\n",
    "\t\tfor time_start in range(0, time_steps, WIKITEXT_TIME_SLICE):\n",
    "\t\t\ttime_end = min(time_start + WIKITEXT_TIME_SLICE, time_steps)\n",
    "\t\t\tlogits_slice = logits[:, time_start:time_end, :].reshape(-1, vocabulary_size)\n",
    "\t\t\ttarget_slice = target_ids[:, time_start:time_end].reshape(-1, 1)\n",
    "\t\t\tadjusted_log_probs = vibe_adjusted_logprobs(logits_slice, vibe_matrix, lambda_value)\n",
    "\t\t\tnegative_log_probs = -adjusted_log_probs.gather(dim=-1, index=target_slice).squeeze(-1)\n",
    "\t\t\tnegative_log_likelihood_sum += float(negative_log_probs.sum().cpu().item())\n",
    "\t\t\ttoken_count_total += int(negative_log_probs.numel())\n",
    "\tif token_count_total == 0:\n",
    "\t\treturn float(\"inf\")\n",
    "\treturn float(math.exp(negative_log_likelihood_sum / token_count_total))\n",
    "\n",
    "set_seed(GLOBAL_SEED)\n",
    "maybe_configure_hf_auth()\n",
    "dataset_wikitext = load_dataset(WIKITEXT_DATASET_ID, WIKITEXT_CONFIG, token=HF_TOKEN)\n",
    "verify_dataset_structure(dataset_wikitext, WIKITEXT_SPLIT)\n",
    "validation_split = dataset_wikitext[WIKITEXT_SPLIT]\n",
    "if MAX_ROWS_EXPERIMENT_4 and MAX_ROWS_EXPERIMENT_4 > 0:\n",
    "\trow_count = min(MAX_ROWS_EXPERIMENT_4, len(validation_split))\n",
    "\tvalidation_split = validation_split.select(range(row_count))\n",
    "texts = [example[\"text\"] for example in validation_split if isinstance(example.get(\"text\"), str)]\n",
    "raw_corpus = \"\\n\".join(texts).strip()\n",
    "require(len(raw_corpus) > 0, \"Empty dataset slice.\")\n",
    "experiment_4_output: Dict[str, object] = {\n",
    "\t\"experiment_id\": 4,\n",
    "\t\"task\": \"WikiText-2 perplexity with vibe-adjusted decoding.\",\n",
    "\t\"lambda_values\": [float(value) for value in LAMBDA_VALUES],\n",
    "\t\"random_seed\": GLOBAL_SEED,\n",
    "\t\"dataset\": {\n",
    "\t\t\"id\": WIKITEXT_DATASET_ID,\n",
    "\t\t\"config\": WIKITEXT_CONFIG,\n",
    "\t\t\"split\": WIKITEXT_SPLIT,\n",
    "\t\t\"max_rows\": int(MAX_ROWS_EXPERIMENT_4)\n",
    "\t},\n",
    "\t\"models\": {}\n",
    "}\n",
    "for model_identifier in WIKITEXT_MODELS:\n",
    "\tprint(f\"\\n=== Loading {model_identifier} dtype={WIKITEXT_PREFERRED_DTYPE} ===\")\n",
    "\ttokenizer, model = get_tokenizer_and_model_wikitext(model_identifier, WIKITEXT_PREFERRED_DTYPE)\n",
    "\tvocabulary_size, hidden_dimension, embedding_dtype = check_shapes_and_dtypes(model)\n",
    "\tprint(f\"Embedding matrix V={vocabulary_size} d={hidden_dimension} dtype={embedding_dtype}\")\n",
    "\tvibe_matrix = make_vibe_matrix(model, WIKITEXT_DEVICE)\n",
    "\twith torch.no_grad():\n",
    "\t\tinput_ids = tokenizer(raw_corpus, return_tensors=\"pt\").input_ids\n",
    "\trequire(input_ids.size(1) >= 2, \"Tokenised length too short for perplexity.\")\n",
    "\tmax_length_value = getattr(model.config, \"max_position_embeddings\", 2048) or 2048\n",
    "\tmodel_metrics: Dict[str, float] = {}\n",
    "\tfor lambda_value in LAMBDA_VALUES:\n",
    "\t\tperplexity_value = stride_ppl_with_vibe(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\ttokenizer=tokenizer,\n",
    "\t\t\tvibe_matrix=vibe_matrix,\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tlambda_value=lambda_value,\n",
    "\t\t\tmax_length=max_length_value,\n",
    "\t\t\tstride=min(512, max_length_value)\n",
    "\t\t)\n",
    "\t\tperplexity_string = f\"{perplexity_value:.8f}\"\n",
    "\t\tmodel_metrics[f\"{lambda_value:+g}\"] = float(perplexity_string)\n",
    "\t\tprint(f\"[{model_identifier}] λ={lambda_value:+g} PPL={perplexity_string}\")\n",
    "\texperiment_4_output[\"models\"][model_identifier] = {\n",
    "\t\t\"results\": model_metrics,\n",
    "\t\t\"details\": {\n",
    "\t\t\t\"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "\t\t\t\"device\": WIKITEXT_DEVICE,\n",
    "\t\t\t\"seed\": GLOBAL_SEED,\n",
    "\t\t\t\"python\": platform.python_version(),\n",
    "\t\t\t\"torch\": torch.__version__,\n",
    "\t\t\t\"transformers\": transformers_lib.__version__,\n",
    "\t\t\t\"datasets\": datasets_lib.__version__\n",
    "\t\t}\n",
    "\t}\n",
    "with open(\"experiment-4.json\", \"w\", encoding=\"utf-8\") as file_handle:\n",
    "\tjson.dump(experiment_4_output, file_handle, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
